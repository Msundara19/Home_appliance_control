# Home Appliance Control for Visually & Verbally Impaired

> Gesture-based home automation enabling **2.2 billion+ visually impaired** and **70 million+ deaf/hard of hearing** individuals to control appliances independently.

![Benchmark](https://github.com/Msundara19/Home_appliance_control/actions/workflows/benchmark.yml/badge.svg)
![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

---

## üéØ Problem Statement

Current smart home solutions **exclude millions**:

| Technology | Limitation |
|------------|------------|
| Voice Assistants (Alexa, Siri) | Unusable by deaf/mute users |
| Touchscreen Controls | Inaccessible to visually impaired |
| Traditional Switches | Require precise motor control |

**Our Solution:** Camera-based hand gesture recognition that works for everyone.

---

## üìä Performance Benchmarks

| Metric | Value |
|--------|-------|
| Gesture Recognition Accuracy | **100%** |
| Average End-to-End Latency | **33ms** |
| P50 Latency | 33ms |
| P95 Latency | 48ms |
| P99 Latency | 49ms |

> Run `python benchmark.py` to reproduce results. CI runs automatically on every push.

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        USER'S HOME                               ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         HTTP/WiFi        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ   Webcam    ‚îÇ                          ‚îÇ  Raspberry Pi   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ     +       ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫    ‚îÇ  + Relay Board  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  PC/Laptop  ‚îÇ                          ‚îÇ  + Appliances   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  (Server)   ‚îÇ  ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ  (Client)       ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       Response           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                                          ‚îÇ             ‚îÇ
‚îÇ         ‚ñº                                          ‚ñº             ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ MediaPipe   ‚îÇ                          ‚îÇ  GPIO Control   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ Hand Track  ‚îÇ                          ‚îÇ  PIN 18 ‚Üí Relay ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ + OpenCV    ‚îÇ                          ‚îÇ  ‚Üí Light/Fan    ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

```
Hand Gesture ‚Üí Camera Capture ‚Üí MediaPipe Detection ‚Üí Finger Counting
     ‚îÇ                                                       ‚îÇ
     ‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                              ‚ñº
     ‚îÇ                      Gesture Mapping
     ‚îÇ                     (1=OFF, 2=ON, etc.)
     ‚îÇ                              ‚îÇ
     ‚îÇ                              ‚ñº
     ‚îÇ                      HTTP POST Request
     ‚îÇ                              ‚îÇ
     ‚îÇ                              ‚ñº
     ‚îÇ                     Raspberry Pi Client
     ‚îÇ                              ‚îÇ
     ‚îÇ                              ‚ñº
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  GPIO Pin Toggle ‚Üí Appliance ON/OFF
                                   
                          Total Latency: ~33ms
```

---

## üñêÔ∏è Gesture Mapping

| Fingers | Action | Use Case |
|---------|--------|----------|
| 1 | OFF | Turn off light/fan |
| 2 | ON | Turn on light/fan |
| 3 | VOL+ | Increase brightness/speed |
| 4 | VOL- | Decrease brightness/speed |
| 5 | (Reserved) | Future: Scene selection |

---

## üõ†Ô∏è Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Hand Detection | MediaPipe Hands | 21-point hand landmark detection |
| Image Processing | OpenCV | Camera capture, preprocessing |
| Backend Server | Python | Gesture processing pipeline |
| IoT Client | Flask | REST API on Raspberry Pi |
| Hardware Control | RPi.GPIO | GPIO pin manipulation |
| CI/CD | GitHub Actions | Automated benchmarking |

---

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- Webcam (for gesture server)
- Raspberry Pi (optional, for hardware control)

### Installation

```bash
# Clone repository
git clone https://github.com/Msundara19/Home_appliance_control.git
cd Home_appliance_control

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create config
cat > src/common/config.yaml << EOF
camera_index: 0
raspi_base_url: "http://192.168.1.100:8081"  # Your Pi's IP
simulation_mode: true  # Set false when Pi is connected
request_timeout_sec: 2.5
draw_debug_text: true
EOF
```

### Run Benchmark (No Hardware Needed)

```bash
python benchmark.py
```

### Run Gesture Server (Needs Webcam)

```bash
python -m src.server.hand_gesture_server
```

### Run Raspberry Pi Client (On the Pi)

```bash
python -m src.client.raspi_gpio_client
```

---

## üìÅ Project Structure

```
Home_appliance_control/
‚îú‚îÄ‚îÄ benchmark.py                 # Performance benchmarking script
‚îú‚îÄ‚îÄ benchmark_report.json        # Generated metrics report
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ benchmark.yml        # CI pipeline
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ server/
    ‚îÇ   ‚îî‚îÄ‚îÄ hand_gesture_server.py    # MediaPipe + OpenCV processing
    ‚îú‚îÄ‚îÄ client/
    ‚îÇ   ‚îî‚îÄ‚îÄ raspi_gpio_client.py      # Flask API + GPIO control
    ‚îî‚îÄ‚îÄ common/
        ‚îú‚îÄ‚îÄ config.yaml               # Configuration
        ‚îî‚îÄ‚îÄ helpers.py                # Utility functions
```

---

## üß™ Testing

### Automated CI

Every push triggers GitHub Actions that:
1. Sets up Python 3.12 environment
2. Installs dependencies
3. Runs benchmark suite
4. Uploads `benchmark_report.json` as artifact

### Manual Testing

```bash
# Run full benchmark
python benchmark.py

# Expected output:
# ‚úÖ Gesture Recognition Accuracy: 100%
# ‚úÖ Average Latency: ~33ms
# ‚úÖ P95 Latency: ~48ms
```

---

## üéØ Impact & Accessibility

### Who Benefits

| User Group | Population | How This Helps |
|------------|------------|----------------|
| Visually Impaired | 2.2 billion globally | No need to see buttons/screens |
| Deaf/Hard of Hearing | 70 million globally | No voice commands needed |
| Motor Impairments | 75 million globally | Simple hand gestures vs precise movements |
| Elderly | 700 million globally | Intuitive, natural interface |

### Accessibility Features

- ‚úÖ **No voice required** - Works for deaf/mute users
- ‚úÖ **No screen reading** - Works for blind users  
- ‚úÖ **Large gesture tolerance** - Works for users with tremors
- ‚úÖ **Visual feedback** - On-screen display of detected gesture
- ‚úÖ **Configurable distance** - Works from 0.3m to 2m

---

## üó∫Ô∏è Roadmap

### Phase 1: Core Functionality ‚úÖ
- [x] MediaPipe hand detection
- [x] Finger counting algorithm
- [x] HTTP-based appliance control
- [x] Raspberry Pi GPIO integration
- [x] Performance benchmarking
- [x] CI/CD pipeline

### Phase 2: Enhanced Recognition (Planned)
- [ ] Custom gesture training (thumbs up, peace sign, etc.)
- [ ] Multi-hand support for complex commands
- [ ] Gesture sequences (e.g., swipe left = next device)
- [ ] Ambient light adaptation

### Phase 3: Smart Home Integration (Planned)
- [ ] Home Assistant integration
- [ ] MQTT protocol support
- [ ] Multiple room/device support
- [ ] Voice + gesture hybrid control

### Phase 4: Edge Deployment (Planned)
- [ ] TensorFlow Lite model for Raspberry Pi
- [ ] On-device inference (no PC needed)
- [ ] Battery-powered portable unit
- [ ] < 100ms latency on edge device

### Phase 5: Accessibility Certification (Future)
- [ ] User testing with disability advocacy groups
- [ ] WCAG compliance documentation
- [ ] Partnership with accessibility organizations

---

## üìà Performance Optimization

### Current Bottlenecks

| Stage | Time | % of Total |
|-------|------|------------|
| MediaPipe Detection | ~20ms | 60% |
| Finger Counting | <1ms | 3% |
| HTTP Request | ~10ms | 30% |
| GPIO Toggle | <1ms | 3% |

### Optimization Opportunities

1. **Edge deployment** - Run MediaPipe on Pi 4 (eliminates network latency)
2. **Model quantization** - INT8 inference for 2x speedup
3. **Local WebSocket** - Replace HTTP with WebSocket for <5ms communication

---

## ü§ù Contributing

Contributions welcome! Areas where help is needed:

- [ ] Custom gesture dataset collection
- [ ] TensorFlow Lite conversion
- [ ] Home Assistant plugin
- [ ] Mobile app (React Native)
- [ ] Documentation in other languages

---

## üìÑ License

MIT License - See [LICENSE](LICENSE) for details.

---

## üôè Acknowledgments

- [MediaPipe](https://developers.google.com/mediapipe) - Hand landmark detection
- [OpenCV](https://opencv.org/) - Computer vision library
- [Raspberry Pi Foundation](https://www.raspberrypi.org/) - Hardware platform

---

## üì¨ Contact

**Meenakshi Sridharan Sundaram**  
[GitHub](https://github.com/Msundara19) | [LinkedIn](https://linkedin.com/in/meenakshi-sridharan)

---

<p align="center">
  <i>Built with ‚ù§Ô∏è for accessibility</i>
</p>
